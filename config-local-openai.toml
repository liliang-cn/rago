# Local OpenAI-Compatible Service Configuration
# For use with vLLM, LocalAI, Ollama OpenAI API, or other compatible services

[server]
port = 7127
host = "0.0.0.0"
enable_ui = true

[providers]
default_llm = "openai"
default_embedder = "openai"

[providers.openai]
type = "openai"
api_key = "dummy-key"  # Many local services don't require real keys
base_url = "http://localhost:8000/v1"  # Local service endpoint
llm_model = "meta-llama/Llama-2-7b-chat-hf"
embedding_model = "BAAI/bge-large-en-v1.5"
timeout = "120s"  # Local services may need more time

[sqvect]
db_path = "./data/rag.db"
vector_dim = 1024  # Adjust based on your embedding model
max_conns = 10
batch_size = 100
top_k = 5
threshold = 0.0

[keyword]
index_path = "./data/keyword.bleve"

[chunker]
chunk_size = 500
overlap = 50
method = "sentence"

[ingest]
[ingest.metadata_extraction]
enable = true
llm_model = "meta-llama/Llama-2-7b-chat-hf"

[tools]
enabled = true
max_concurrent_calls = 3  # Lower for local services
call_timeout = "60s"      # Longer timeout for local services
security_level = "normal"
enabled_tools = [
    "datetime",
    "rag_search",
    "document_info",
    "file_operations"
]