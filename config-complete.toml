# RAGO Complete Configuration File
# This configuration showcases the new provider system with all available options

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================
[server]
port = 7127                    # HTTP server port
host = "0.0.0.0"              # Server host (0.0.0.0 allows LAN access)
enable_ui = true              # Enable web UI
cors_origins = ["*"]          # CORS allowed origins

# =============================================================================
# PROVIDER SYSTEM CONFIGURATION (NEW)
# =============================================================================
[providers]
# Default providers to use for different services
default_llm = "ollama"        # Options: "ollama", "openai"
default_embedder = "ollama"   # Options: "ollama", "openai"

# Ollama Provider Configuration
[providers.ollama]
type = "ollama"
base_url = "http://localhost:11434"
llm_model = "qwen3"              # LLM model for text generation
embedding_model = "nomic-embed-text"   # Model for embeddings
timeout = "120s"                       # Request timeout

# OpenAI Provider Configuration (Optional)
[providers.openai]
type = "openai"
api_key = "sk-your-openai-api-key-here"
base_url = "https://api.openai.com/v1"  # Can be changed for compatible services
llm_model = "gpt-4"                     # LLM model for text generation
embedding_model = "text-embedding-3-small"  # Model for embeddings
organization = ""                       # Optional: OpenAI organization ID
project = ""                           # Optional: OpenAI project ID
timeout = "60s"                        # Request timeout

# =============================================================================
# VECTOR DATABASE CONFIGURATION
# =============================================================================
[sqvect]
db_path = "./data/rag.db"      # SQLite database file path
vector_dim = 768               # Vector dimension (must match embedding model)
                               # Ollama nomic-embed-text: 768
                               # OpenAI text-embedding-3-small: 1536
                               # OpenAI text-embedding-3-large: 3072
max_conns = 10                 # Maximum database connections
batch_size = 100               # Batch size for operations
top_k = 5                      # Default number of chunks to retrieve
threshold = 0.0                # Similarity threshold for filtering

# =============================================================================
# KEYWORD SEARCH CONFIGURATION
# =============================================================================
[keyword]
index_path = "./data/keyword.bleve"  # Bleve keyword index path

# =============================================================================
# RECIPROCAL RANK FUSION (RRF) CONFIGURATION
# =============================================================================
[rrf]
k = 10                         # RRF constant (lower = less score compression)
relevance_threshold = 0.05     # Threshold for considering context relevant

# =============================================================================
# DOCUMENT CHUNKING CONFIGURATION
# =============================================================================
[chunker]
chunk_size = 500               # Maximum chunk size in tokens
overlap = 50                   # Overlap between chunks in tokens
method = "sentence"            # Chunking method: "sentence", "paragraph", "token"

# =============================================================================
# DOCUMENT INGESTION CONFIGURATION
# =============================================================================
[ingest]
[ingest.metadata_extraction]
enable = true                  # Enable automatic metadata extraction
llm_model = "qwen3"      # LLM model for metadata extraction

# =============================================================================
# TOOLS CONFIGURATION
# =============================================================================
[tools]
enabled = true                 # Enable tool calling functionality
max_concurrent_calls = 5       # Maximum concurrent tool executions
call_timeout = "30s"           # Timeout for individual tool calls
security_level = "normal"      # Security level: "strict", "normal", "permissive"
log_level = "info"            # Tool logging level: "debug", "info", "warn", "error"

# Enabled tools list
enabled_tools = [
    "datetime",                # Date and time utilities
    "rag_search",             # RAG search within knowledge base
    "document_info",          # Document information queries
    "file_operations",        # File system operations
    "http_request",           # HTTP request tool
    "open_url",            # Web scraping tool
    "web_search",          # Google search integration
]

# Rate limiting configuration
[tools.rate_limit]
calls_per_minute = 60         # Maximum calls per minute
calls_per_hour = 1000         # Maximum calls per hour
burst_size = 10               # Burst capacity

# Built-in tools configuration
[tools.builtin]
[tools.builtin.datetime]
enabled = true
formats = ["2006-01-02", "2006-01-02 15:04:05", "RFC3339"]

[tools.builtin.file_operations]
enabled = true
allowed_paths = ["./data", "./knowledge", "./examples"]  # Restrict file access
max_file_size = "10MB"

[tools.builtin.http_request]
enabled = true
timeout = "30s"
max_redirects = 5
allowed_schemes = ["http", "https"]

[tools.builtin.search]
enabled = true
max_results = 10
cache_ttl = "1h"

# Plugin system configuration
[tools.plugins]
enabled = false               # Enable plugin system
plugin_paths = ["./plugins"]  # Plugin search paths
auto_load = true             # Automatically load plugins at startup

# =============================================================================
# LEGACY OLLAMA CONFIGURATION (BACKWARD COMPATIBILITY)
# =============================================================================
# Note: This section is maintained for backward compatibility.
# New installations should use the [providers] section above.
[ollama]
base_url = "http://localhost:11434"
llm_model = "qwen3"
embedding_model = "nomic-embed-text"
timeout = "120s"