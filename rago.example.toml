# RAGO v2 Configuration Example
# This file demonstrates all available configuration options for RAGO v2
# Copy this file to 'rago.toml' and modify according to your needs

[server]
# HTTP server configuration
port = 7127                           # Server port (avoids common ports like 3000, 8080, 9000)
host = "0.0.0.0"                     # Server host (0.0.0.0 for LAN access)
enable_ui = false                     # Enable web UI (not implemented in v2)
cors_origins = ["*"]                  # CORS origins for API

[providers]
# Provider configuration
default_llm = "openai"               # Default LLM provider
default_embedder = "openai"           # Default embedding provider

[providers.openai]
# OpenAI-compatible provider configuration
# Works with: OpenAI API, Ollama, LM Studio, and any OpenAI-compatible service
type = "openai"
base_url = "http://localhost:11434/v1"     # LLM endpoint URL
api_key = ""                                # API key (optional - provider handles auth)
llm_model = "qwen3"                        # LLM model name
embedding_model = "nomic-embed-text"       # Embedding model name
timeout = "30s"                            # Request timeout
# organization = "your-org-id"             # Optional: OpenAI organization
# project = "your-project-id"               # Optional: OpenAI project

[sqvect]
# SQLite vector database configuration
db_path = "~/.rago/data/rag.db"            # Database file path
max_conns = 10                             # Maximum connections
batch_size = 100                           # Batch size for operations
top_k = 5                                  # Number of results to return
threshold = 0.0                            # Similarity threshold (0.0-1.0)

[chunker]
# Text chunking configuration
chunk_size = 500                           # Size of text chunks
overlap = 50                              # Overlap between chunks
method = "sentence"                        # Chunking method: "sentence", "paragraph", "token"

[ingest.metadata_extraction]
# Automatic metadata extraction configuration
enable = false                             # Enable metadata extraction
llm_model = ""                             # LLM model for extraction (auto-configured)

[mcp]
# Model Context Protocol (MCP) configuration
enabled = true                             # Enable MCP tool integration
log_level = "info"                         # MCP log level: "debug", "info", "warn", "error"
default_timeout = "30s"                    # Default timeout for MCP operations
max_concurrent_requests = 5                # Maximum concurrent MCP requests
health_check_interval = "60s"              # Health check interval
servers_config_path = "mcpServers.json"    # MCP servers configuration file

# MCP Servers Configuration (External JSON file)
# Create mcpServers.json with the following structure:
# {
#   "filesystem": {
#     "command": "npx",
#     "args": ["@modelcontextprotocol/server-filesystem", "/path/to/allowed/directory"],
#     "description": "File system operations"
#   },
#   "fetch": {
#     "command": "npx",
#     "args": ["@modelcontextprotocol/server-fetch"],
#     "description": "HTTP/HTTPS operations"
#   },
#   "memory": {
#     "command": "npx",
#     "args": ["@modelcontextprotocol/server-memory"],
#     "description": "Memory storage operations"
#   }
# }

# Advanced Configuration Options

[vector_store]
# Optional: External vector store configuration
# type = "qdrant"                         # Vector store type: "sqlite" (default), "qdrant"
# parameters = {                           # Store-specific parameters
#   "host": "localhost",
#   "port": 6333,
#   "api_key": "your-qdrant-key"
# }

# Environment Variable Override Examples
# Set these in your environment to override config values:
#
# export RAGO_SERVER_PORT="7127"
# export RAGO_SERVER_HOST="0.0.0.0"
# export RAGO_PROVIDERS_DEFAULT_LLM="openai"
# export RAGO_PROVIDERS_DEFAULT_EMBEDDER="openai"
# export RAGO_OPENAI_API_KEY="your-api-key"
# export RAGO_OPENAI_BASE_URL="http://localhost:11434/v1"
# export RAGO_OPENAI_LLM_MODEL="qwen3"
# export RAGO_OPENAI_EMBEDDING_MODEL="nomic-embed-text"
# export RAGO_SQVECT_DB_PATH="/custom/path/rag.db"
# export RAGO_CHUNKER_CHUNK_SIZE="500"
# export RAGO_CHUNKER_OVERLAP="50"
# export RAGO_CHUNKER_METHOD="sentence"
# export RAGO_MCP_ENABLED="true"

# Provider-Specific Examples

## Ollama Configuration (Local)
# [providers.openai]
# type = "openai"
# base_url = "http://localhost:11434/v1"
# api_key = ""  # Optional
# llm_model = "qwen3"
# embedding_model = "nomic-embed-text"

## LM Studio Configuration (Local)
# [providers.openai]
# type = "openai"
# base_url = "http://localhost:1234/v1"
# api_key = ""  # Optional
# llm_model = "your-model-name"
# embedding_model = "your-embedding-model"

## OpenAI Configuration (Cloud)
# [providers.openai]
# type = "openai"
# base_url = "https://api.openai.com/v1"
# api_key = "sk-your-openai-api-key"  # Set if needed
# llm_model = "gpt-4o-mini"
# embedding_model = "text-embedding-3-small"
# organization = "org-your-org-id"

## Custom OpenAI-Compatible Service
# [providers.openai]
# type = "openai"
# base_url = "https://your-service.com/v1"
# api_key = "your-api-key"  # Set if needed
# llm_model = "your-model"
# embedding_model = "your-embedding-model"