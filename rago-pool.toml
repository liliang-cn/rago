# RAGO - LLM Pool Configuration Example
# This configuration demonstrates using multiple LLM providers in a pool

[providers]
# When llm_pool is enabled, the pool will be used instead of default_llm
default_llm = "ollama"
default_embedder = "ollama"

# Configure multiple LLM providers
[providers.ollama]
base_url = "http://localhost:11434"
llm_model = "qwen3:8b"
embedding_model = "nomic-embed-text"
timeout = "120s"

[providers.lmstudio]
base_url = "http://localhost:1234"
llm_model = "qwen/qwen3-4b-2507"
embedding_model = "text-embedding-qwen3-embedding-4b"
timeout = "120s"

[providers.openai]
base_url = "https://api.openai.com/v1"
api_key = "${OPENAI_API_KEY}"  # Set via environment variable
llm_model = "gpt-3.5-turbo"
embedding_model = "text-embedding-ada-002"
timeout = "60s"

# LLM Pool Configuration
[providers.llm_pool]
enabled = true
providers = ["ollama", "lmstudio"]  # Use both Ollama and LM Studio
strategy = "round_robin"            # Options: round_robin, random, least_load, failover
health_check_interval = "30s"       # Check provider health every 30 seconds
max_retries = 2                     # Retry failed requests up to 2 times
retry_delay = "1s"                  # Wait 1 second between retries

[mcp]
enabled = true
servers = ["./mcpServers.json"]
log_level = "info"
default_timeout = "60s"
max_concurrent_requests = 10
health_check_interval = "60s"